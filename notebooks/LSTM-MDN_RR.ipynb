{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General Description**\n",
    "\n",
    "The following notebook contains the code to create, train, validate, and test a rainfall-runoff model using the LSTM-MDN network architecture. The notebook support running experiments in different large-sample hydrology datasets including: CAMELS-GB, CAMELS-US, CAMELS-DE. The details for each dataset can be read from a .yml file.\n",
    "\n",
    "***Authors:***\n",
    "- Eduardo Acuña Espinoza (eduardo.espinoza@kit.edu)\n",
    "- Manuel Alvarez Chaves (manuel.alvarez-chaves@simtech.uni-stuttgart.de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import xarray as xr\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hy2dl.datasetzoo import get_dataset\n",
    "from hy2dl.modelzoo import get_model\n",
    "from hy2dl.training.loss import loss_nll\n",
    "from hy2dl.utils.config import Config\n",
    "from hy2dl.utils.optimizer import Optimizer\n",
    "from hy2dl.utils.utils import set_random_seed, upload_to_device\n",
    "\n",
    "# colorblind friendly palette\n",
    "color_palette = {\"observed\": \"#377eb8\",\"simulated\": \"#4daf4a\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1. Initialize information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results\\\\LSTMGMM-250-01_NLL-GAUSSIAN_S05_010\\\\config.yml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# path_experiment_settings = \"../results/LSTMGMM-250-01_NLL-GAUSSIAN_S42_010/config.yml\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Read experiment settings\u001b[39;00m\n\u001b[0;32m      5\u001b[0m config \u001b[38;5;241m=\u001b[39m Config(path_experiment_settings, dev_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\SimTech\\Projects\\Hy2DL\\src\\hy2dl\\utils\\config.py:62\u001b[0m, in \u001b[0;36mConfig.dump\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m         temp_cfg[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_save_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig.yml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     63\u001b[0m     yaml\u001b[38;5;241m.\u001b[39mdump(temp_cfg, file, default_flow_style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, sort_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results\\\\LSTMGMM-250-01_NLL-GAUSSIAN_S05_010\\\\config.yml'"
     ]
    }
   ],
   "source": [
    "# Path to .yml file where the experiment settings are stored. The experimet settings can also be defined manually as a dictionary.\n",
    "path_experiment_settings = \"../examples/mdn.yml\"\n",
    "# path_experiment_settings = \"../results/LSTMGMM-250-01_NLL-GAUSSIAN_S42_010/config.yml\"\n",
    "# Read experiment settings\n",
    "config = Config(path_experiment_settings, dev_mode=True)\n",
    "config.dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2. Create datasets and dataloaders used to train/validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset class\n",
    "Dataset = get_dataset(config)\n",
    "\n",
    "# Dataset training\n",
    "config.logger.info(f\"Loading training data from {config.dataset} dataset\")\n",
    "total_time = time.time()\n",
    "\n",
    "training_dataset = Dataset(cfg=config, \n",
    "                           time_period=\"training\")\n",
    "\n",
    "training_dataset.calculate_basin_std()\n",
    "training_dataset.calculate_global_statistics(save_scaler=True)\n",
    "training_dataset.standardize_data()\n",
    "\n",
    "config.logger.info(f\"Number of entities with valid samples: {len(training_dataset.df_ts)}\")\n",
    "config.logger.info(f\"Time required to process {len(training_dataset.df_ts)} entities: {datetime.timedelta(seconds=int(time.time()-total_time))}\")\n",
    "config.logger.info(f\"Number of valid training samples: {len(training_dataset)}\\n\")\n",
    "\n",
    "# Dataloader training\n",
    "train_loader = DataLoader(dataset=training_dataset,\n",
    "                          batch_size=config.batch_size_training,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True,\n",
    "                          collate_fn=training_dataset.collate_fn,\n",
    "                          num_workers=config.num_workers)\n",
    "\n",
    "# Print details of a loader´s sample to check that the format is correct\n",
    "config.logger.info(\"Details training dataloader\".center(60, \"-\"))\n",
    "config.logger.info(f\"Batch structure (number of batches: {len(train_loader)})\")\n",
    "config.logger.info(f\"{'Key':^30}|{'Shape':^30}\")\n",
    "\n",
    "# Loop through the sample dictionary and print the shape of each element\n",
    "for key, value in next(iter(train_loader)).items():\n",
    "    if key.startswith((\"x_d\", \"x_conceptual\")):\n",
    "        config.logger.info(f\"{key}\")\n",
    "        for i, v in value.items():\n",
    "            config.logger.info(f\"{i:^30}|{str(v.shape):^30}\")\n",
    "    else:\n",
    "        config.logger.info(f\"{key:<30}|{str(value.shape):^30}\")\n",
    "        \n",
    "config.logger.info(\"\")  # prints a blank line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.logger.info(f\"Loading validation data from {config.dataset} dataset\")\n",
    "\n",
    "# Validate on random basins\n",
    "if config.validate_n_random_basins > 0:\n",
    "    entities_ids = np.loadtxt(config.path_entities, dtype=\"str\").tolist()\n",
    "    random_basins = np.random.choice(entities_ids, size=config.validate_n_random_basins, replace=False).tolist()\n",
    "\n",
    "total_time = time.time()\n",
    "validation_dataset = Dataset(cfg=config,\n",
    "                      time_period=\"validation\",\n",
    "                      entities_ids=random_basins if config.validate_n_random_basins > 0 else None)\n",
    "    \n",
    "validation_dataset.scaler = training_dataset.scaler\n",
    "validation_dataset.standardize_data()\n",
    "\n",
    "config.logger.info(f\"Time required to process {len(validation_dataset.df_ts)} entities: {datetime.timedelta(seconds=int(time.time()-total_time))}\\n\")\n",
    "config.logger.info(f\"Number of validation samples: {len(validation_dataset)}\\n\")\n",
    "\n",
    "validation_loader = DataLoader(dataset=validation_dataset,\n",
    "                                  batch_size=config.batch_size_evaluation,\n",
    "                                  shuffle=False,\n",
    "                                  drop_last=False,\n",
    "                                  collate_fn=validation_dataset.collate_fn,\n",
    "                                  num_workers=config.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "set_random_seed(cfg=config)\n",
    "model = get_model(config).to(config.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "set_random_seed(cfg=config)\n",
    "model = get_model(config).to(config.device)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = Optimizer(cfg=config, model=model) \n",
    "\n",
    "# Training report structure\n",
    "config.logger.info(\"Training model\".center(60, \"-\"))\n",
    "config.logger.info(f\"{'':^16}|{'Trainining':^21}|{'Validation':^21}|\")\n",
    "config.logger.info(f\"{'Epoch':^5}|{'LR':^10}|{'Loss':^10}|{'Time':^10}|{'Metric':^10}|{'Time':^10}|\")\n",
    "\n",
    "total_time = time.time()\n",
    "# Loop through epochs\n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    train_time = time.time()\n",
    "    loss_evol = []\n",
    "    # Training -------------------------------------------------------------------------------------------------------\n",
    "    model.train()\n",
    "    # Loop through the different batches in the training dataset\n",
    "    iterator = tqdm(train_loader, \n",
    "                    desc=f\"Epoch {epoch}/{config.epochs}. Training\", \n",
    "                    unit=\"batches\", \n",
    "                    ascii=True, \n",
    "                    leave=False)\n",
    "    \n",
    "    for idx, sample in enumerate(iterator):\n",
    "        # reach maximum iterations per epoch\n",
    "        if config.max_updates_per_epoch is not None and idx >= config.max_updates_per_epoch:\n",
    "            break\n",
    "\n",
    "        sample = upload_to_device(sample, config.device)  # upload tensors to device\n",
    "        optimizer.optimizer.zero_grad()  # sets gradients to zero\n",
    "        \n",
    "        # Forward pass of the model\n",
    "        pred = model(sample)\n",
    "        loss = loss_nll(\n",
    "            params=pred[\"params\"],\n",
    "            weights=pred[\"weights\"],\n",
    "            dist=model.distribution,\n",
    "            y_obs=sample[\"y_obs\"]\n",
    "        )\n",
    "        loss = loss.sum()\n",
    "        \n",
    "        # Backpropagation (calculate gradients)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update model parameters (e.g, weights and biases)\n",
    "        optimizer.clip_grad_and_step(epoch, idx)\n",
    "\n",
    "        # Keep track of the loss per batch\n",
    "        loss_evol.append(loss.item())\n",
    "        iterator.set_postfix({\"loss\": f\"{np.mean(loss_evol):.3f}\"})\n",
    "\n",
    "        # remove elements from cuda to free memory\n",
    "        del sample, pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # training report\n",
    "    report = f'{epoch:^5}|{optimizer.optimizer.param_groups[0][\"lr\"]:^10.5f}|{np.mean(loss_evol):^10.3f}|{str(datetime.timedelta(seconds=int(time.time()-train_time))):^10}|'\n",
    "\n",
    "    # Validation -----------------------------------------------------------------------------------------------------\n",
    "    if epoch % config.validate_every == 0:\n",
    "        val_time = time.time()\n",
    "        model.eval()\n",
    "\n",
    "        iterator = tqdm(validation_loader, \n",
    "                desc=f\"Epoch {epoch}/{config.epochs}. Validation\", \n",
    "                unit=\"batches\", \n",
    "                ascii=True, \n",
    "                leave=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss_evol = []\n",
    "            for idx, sample in enumerate(iterator):\n",
    "                sample = upload_to_device(sample, config.device)\n",
    "                pred = model(sample)\n",
    "                loss = loss_nll(\n",
    "                    params=pred[\"params\"],\n",
    "                    weights=pred[\"weights\"],\n",
    "                    dist=model.distribution,\n",
    "                    y_obs=sample[\"y_obs\"]\n",
    "                )\n",
    "                loss = loss.sum()\n",
    "                loss_evol.append(loss.item())\n",
    "                iterator.set_postfix({\"loss\": f\"{np.nanmean(loss_evol):.3f}\"})\n",
    "\n",
    "            # average loss validation\n",
    "            report += f\"{np.nanmean(loss_evol):^10.3f}|{str(datetime.timedelta(seconds=int(time.time()-val_time))):^10}|\"\n",
    "\n",
    "    # No validation\n",
    "    else:\n",
    "        report += f\"{'':^10}|{'':^10}|\"\n",
    "    \n",
    "\n",
    "    # Print report and save model\n",
    "    config.logger.info(report)\n",
    "    torch.save(model.state_dict(), config.path_save_folder / \"model\" / f\"model_epoch_{epoch}\")\n",
    "    # modify learning rate\n",
    "    optimizer.update_optimizer_lr(epoch=epoch)\n",
    "\n",
    "# print total training time\n",
    "config.logger.info(f'Total training time: {datetime.timedelta(seconds=int(time.time()-total_time))}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case I already trained an LSTM I can re-construct the model. I just need to define the epoch for which I want to\n",
    "# re-construct the model\n",
    "# model = get_model(config).to(config.device)\n",
    "# model.load_state_dict(torch.load(config.path_save_folder / \"model\" / \"model_epoch_10\", map_location=config.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read previously generated scaler\n",
    "with open(config.path_save_folder / \"scaler.pickle\", \"rb\") as file:\n",
    "    scaler = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_location = config.path_save_folder.parent / \"LSTMGMM-250-01_NLL-GAUSSIAN_S42_531\"\n",
    "\n",
    "model = get_model(config).to(config.device)\n",
    "model.load_state_dict(torch.load(other_location / \"model\" / \"model_epoch_10\", map_location=config.device))\n",
    "\n",
    "with open(other_location / \"scaler.pickle\", \"rb\") as file:\n",
    "    scaler = pickle.load(file)\n",
    "\n",
    "# Get dataset class\n",
    "Dataset = get_dataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In evaluation (validation and testing) we will create an individual dataset per basin\n",
    "config.logger.info(f\"Loading testing data from {config.dataset} dataset\")\n",
    "\n",
    "entities_ids = np.loadtxt(config.path_entities_testing, dtype=\"str\").tolist()\n",
    "iterator = tqdm([entities_ids] if isinstance(entities_ids, str) else entities_ids, \n",
    "                desc=\"Processing entities\", \n",
    "                unit=\"entity\", \n",
    "                ascii=True)\n",
    "\n",
    "total_time = time.time()\n",
    "testing_dataset = {}\n",
    "for entity in iterator:\n",
    "    dataset = Dataset(cfg= config, \n",
    "                      time_period= \"testing\",\n",
    "                      check_NaN=False,\n",
    "                      entities_ids=entity)\n",
    "\n",
    "    dataset.scaler = scaler\n",
    "    dataset.standardize_data(standardize_output=False)\n",
    "    testing_dataset[entity] = dataset\n",
    "\n",
    "config.logger.info(f\"Time required to process {len(iterator)} entities: {datetime.timedelta(seconds=int(time.time()-total_time))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.logger.info(\"Testing model\".center(60, \"-\"))\n",
    "total_time = time.time()\n",
    "\n",
    "model.eval()\n",
    "out = {}\n",
    "with torch.no_grad():\n",
    "    # Go through each basin\n",
    "    iterator = tqdm(testing_dataset, desc=f\"Testing\", unit=\"basins\", ascii=True)\n",
    "    for basin in iterator:\n",
    "        loader = DataLoader(\n",
    "            dataset=testing_dataset[basin],\n",
    "            batch_size=config.batch_size_evaluation,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            collate_fn=testing_dataset[basin].collate_fn,\n",
    "            num_workers=config.num_workers\n",
    "        )\n",
    "\n",
    "        dates, y_obs, y_hat, params, weights = [], [], [], {}, []\n",
    "        for sample in loader:\n",
    "            sample = upload_to_device(sample, config.device)  # upload tensors to device\n",
    "\n",
    "            dates.append(sample[\"date\"])\n",
    "            y_obs.append(sample[\"y_obs\"].detach().cpu().numpy())\n",
    "\n",
    "            # Generate predictions\n",
    "            batch_params, batch_weights = model(sample).values()\n",
    "            for k, v in batch_params.items():\n",
    "                params[k] = params.get(k, []) + [v.detach().cpu().numpy()]\n",
    "\n",
    "            weights.append(batch_weights.detach().cpu().numpy())\n",
    "\n",
    "            pred = model.sample(sample, 1)\n",
    "            pred[:, :, 0, :] = model.mean(sample)\n",
    "\n",
    "            # backtransformed information\n",
    "            pred = pred * testing_dataset[basin].scaler[\"y_std\"].to(config.device) + (testing_dataset[basin].scaler[\"y_mean\"].to(config.device))\n",
    "            y_hat.append(pred.detach().cpu().numpy())\n",
    "\n",
    "            # remove from cuda\n",
    "            del sample, pred\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        out[basin] = {\n",
    "            \"date\": np.concatenate(dates),\n",
    "            \"y_obs\": np.concatenate(y_obs),\n",
    "            \"y_hat\": np.concatenate(y_hat),\n",
    "            \"params\": {k: np.concatenate(v) for k, v in params.items()},\n",
    "            \"weights\": np.concatenate(weights)\n",
    "        }\n",
    "        del dates, y_obs, y_hat, params, weights\n",
    "\n",
    "\n",
    "config.logger.info(f'Total testing time: {datetime.timedelta(seconds=int(time.time()-total_time))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "            dataset=testing_dataset[\"13011900\"],\n",
    "            batch_size=config.batch_size_evaluation,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            collate_fn=testing_dataset[\"13011900\"].collate_fn,\n",
    "            num_workers=config.num_workers\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mean = testing_dataset[basin].scaler[\"y_mean\"].to(config.device)\n",
    "y_std = testing_dataset[basin].scaler[\"y_std\"].to(config.device)\n",
    "\n",
    "dates = sample[\"date\"][:, -1]\n",
    "y_obs = sample[\"y_obs\"][:, -1, 0]\n",
    "mean = (model.mean(sample)[:, -1, 0] * y_std + y_mean)\n",
    "\n",
    "samples = (model.sample(sample, num_samples=1000)[:, 0, :, 0] * y_std + y_mean).detach()\n",
    "\n",
    "y_obs_std = (y_obs - y_mean) / y_std\n",
    "logpdf = model._calc_logpdf(sample, y_obs_std)[:, -1, 0]\n",
    "\n",
    "cdf = model._calc_cdf(sample, y_obs_std)[:, -1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile = model.quantile(sample, [0.025, 0.5, 0.975])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_025 = (quantile[:, 0, 0, 0] * y_std + y_mean).numpy()\n",
    "q_50 = (quantile[:, 0, 1, 0] * y_std + y_mean).numpy()\n",
    "q_975 = (quantile[:, 0, 2, 0] * y_std + y_mean).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(dates, y_obs, label=\"Observed\", color=\"C0\")\n",
    "ax.plot(dates, q_50, label=\"Quantile 0.50\", color=\"C2\")\n",
    "ax.fill_between(dates, q_025, q_975, color=\"C2\", alpha=0.2, label=\"Quantile 0.025 - 0.975\")\n",
    "ax.set_ylim(-0.5, 17.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(dates, mean - q_50, label=\"Mean - Quantile 0.50\", color=\"C3\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(dates, y_obs - y_obs, label=\"Observed\", color=\"C0\")\n",
    "ax.plot(dates, mean - y_obs, label=\"Predicted\", color=\"C1\")\n",
    "ax.fill_between(dates, samples.quantile(0.025, axis=1) - y_obs, samples.quantile(0.975, axis=1) - y_obs, color=\"C1\", alpha=0.2)\n",
    "ax.set_ylim(-4, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(dates, logpdf.detach().numpy(), label=\"Log-probability\", color=\"C0\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(dates, cdf.detach().numpy(), label=\"CDF\", color=\"C0\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_xarray(out_dict):\n",
    "    # Get all basin IDs\n",
    "    basin_ids = list(out_dict.keys())\n",
    "    \n",
    "    # Get dates from first basin (assuming all basins have same dates)\n",
    "    dates = out_dict[basin_ids[0]]['date']\n",
    "    num_days = dates.shape[0]\n",
    "    \n",
    "    # Get dimensions from the first basin\n",
    "    first_basin = out_dict[basin_ids[0]]\n",
    "    num_basins = len(basin_ids)\n",
    "    predict_last_n = first_basin['y_obs'].shape[1]\n",
    "    num_targets = first_basin['y_obs'].shape[2]\n",
    "    num_samples = first_basin['y_hat'].shape[2]\n",
    "    \n",
    "    # Get parameter names dynamically from params subdictionary\n",
    "    param_names = list(first_basin['params'].keys())\n",
    "    num_components = first_basin['params'][param_names[0]].shape[2]\n",
    "    \n",
    "    # Initialize arrays for fixed variables\n",
    "    y_obs_array = np.zeros((num_basins, num_days, predict_last_n, num_targets))\n",
    "    y_hat_array = np.zeros((num_basins, num_days, predict_last_n, num_samples, num_targets))\n",
    "    weights_array = np.zeros((num_basins, num_days, predict_last_n, num_components, num_targets))\n",
    "    \n",
    "    # Initialize arrays for parameter variables dynamically\n",
    "    param_arrays = {}\n",
    "    for param_name in param_names:\n",
    "        param_arrays[param_name] = np.zeros((num_basins, num_days, predict_last_n, num_components, num_targets))\n",
    "    \n",
    "    # Fill arrays\n",
    "    for idx, basin_id in enumerate(basin_ids):\n",
    "        basin_data = out_dict[basin_id]\n",
    "\n",
    "        y_obs_array[idx] = basin_data['y_obs']\n",
    "        y_hat_array[idx] = basin_data['y_hat']\n",
    "        weights_array[idx] = basin_data['weights']\n",
    "        \n",
    "        # Fill parameter arrays dynamically\n",
    "        for param_name in param_names:\n",
    "            param_arrays[param_name][idx] = basin_data['params'][param_name]\n",
    "\n",
    "    # Create coordinate arrays\n",
    "    coords = {\n",
    "        'basin_id': basin_ids,\n",
    "        'date': dates[:, -1],\n",
    "        'predict_last_n': np.arange(predict_last_n),\n",
    "        'num_targets': np.arange(num_targets),\n",
    "        'num_samples': np.arange(num_samples),\n",
    "        'num_components': np.arange(num_components)\n",
    "    }\n",
    "    \n",
    "    # Create data variables\n",
    "    data_vars = {\n",
    "        'y_obs': (['basin_id', 'date', 'predict_last_n', 'num_targets'], y_obs_array),\n",
    "        'y_hat': (['basin_id', 'date', 'predict_last_n', 'num_samples', 'num_targets'], y_hat_array),\n",
    "        'weights': (['basin_id', 'date', 'predict_last_n', 'num_components', 'num_targets'], weights_array)\n",
    "    }\n",
    "    \n",
    "    # Add parameter variables dynamically\n",
    "    for param_name in param_names:\n",
    "        data_vars[param_name] = (['basin_id', 'date', 'predict_last_n', 'num_components', 'num_targets'], param_arrays[param_name])\n",
    "    \n",
    "    # Create the dataset\n",
    "    ds = xr.Dataset(data_vars, coords=coords)\n",
    "    \n",
    "    # Add attributes for better documentation\n",
    "    ds.attrs['description'] = 'Basin prediction data with observations, predictions, and model parameters'\n",
    "    ds['y_obs'].attrs['description'] = 'Observed values'\n",
    "    ds['y_hat'].attrs['description'] = 'Predicted values (ensemble samples)'\n",
    "    ds['weights'].attrs['description'] = 'Weight values'\n",
    "    \n",
    "    # Add parameter descriptions dynamically\n",
    "    for param_name in param_names:\n",
    "        ds[param_name].attrs['description'] = f'{param_name.capitalize()} parameters'\n",
    "    \n",
    "    return ds\n",
    "\n",
    "ds = to_xarray(out)\n",
    "path_experiment = config.path_save_folder / f\"rest_results.nc\"\n",
    "compression = {var: {\"zlib\":True, \"complevel\":5, \"dtype\":\"f4\"} for var in ds.data_vars}\n",
    "ds.to_netcdf(path_experiment, encoding=compression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 5. Initial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data\n",
    "basin_to_analyze = \"13011900\"\n",
    "data = ds.sel(basin_id=basin_to_analyze).sel(date=slice(\"1991-02-20\", \"1991-11-15\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, figsize=(10, 8))\n",
    "\n",
    "for idx, ax in enumerate(axes):\n",
    "    ax.plot(data[\"date\"], data[\"y_obs\"][:, 0, idx], label=\"Observed\", color=color_palette[\"observed\"])\n",
    "    ax.plot(data[\"date\"], data[\"y_hat\"].median(dim=\"num_samples\")[:, 0, idx], label=\"Simulated\", color=color_palette[\"simulated\"])\n",
    "    ax.fill_between(data[\"date\"], data[\"y_hat\"].quantile(0.05, dim=\"num_samples\")[:, 0, idx], data[\"y_hat\"].quantile(0.95, dim=\"num_samples\")[:, 0, idx], color=color_palette[\"simulated\"], alpha=0.35)\n",
    "    ax.grid(ls=\"--\", alpha=0.5)\n",
    "\n",
    "# Format plot\n",
    "axes[0].legend()\n",
    "axes[0].set_ylabel(\"Streamflow [mm/day]\")\n",
    "axes[1].set_ylabel(\"Vapor pressure [Pa]\")\n",
    "axes[1].set_xlabel(\"Date\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m111"
  },
  "kernelspec": {
   "display_name": "hy2dl (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
