{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a367bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from hy2dl.modelzoo.inputlayer import InputLayer\n",
    "from hy2dl.utils.config import Config\n",
    "from hy2dl.utils.distributions import Distribution\n",
    "\n",
    "path_experiment_settings = \"../examples/mdn.yml\"\n",
    "\n",
    "config = Config(path_experiment_settings, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc55b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMMDN(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_net = InputLayer(cfg)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_net.output_size, hidden_size=cfg.hidden_size, batch_first=True)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=cfg.dropout_rate)\n",
    "\n",
    "        self.distribution = Distribution.from_string(cfg.distribution)\n",
    "        match self.distribution:\n",
    "            case Distribution.GAUSSIAN:\n",
    "                self.num_params = 2\n",
    "            case Distribution.LAPLACIAN:\n",
    "                self.num_params = 3\n",
    "\n",
    "        self.fc_params = nn.Linear(cfg.hidden_size, self.num_params * cfg.num_components * cfg.output_features)\n",
    "\n",
    "        self.fc_weights = nn.Sequential(\n",
    "            nn.Linear(cfg.hidden_size, cfg.num_components * cfg.output_features),\n",
    "            nn.Unflatten(-1, (cfg.num_components, cfg.output_features)),\n",
    "            nn.Softmax(dim=-2)\n",
    "        )\n",
    "\n",
    "        self.num_components = cfg.num_components\n",
    "        self.predict_last_n = cfg.predict_last_n\n",
    "\n",
    "        self.output_features = cfg.output_features\n",
    "\n",
    "        self._reset_parameters(cfg=cfg)\n",
    "\n",
    "    def _reset_parameters(self, cfg: Config):\n",
    "        \"\"\"Special initialization of the bias.\"\"\"\n",
    "        if cfg.initial_forget_bias is not None:\n",
    "            self.lstm.bias_hh_l0.data[cfg.hidden_size : 2 * cfg.hidden_size] = cfg.initial_forget_bias\n",
    "    \n",
    "    def forward(self, sample):\n",
    "        # Pre-process data to be sent to the LSTM\n",
    "        # processed_sample = self.embedding_net(sample)\n",
    "        # x_lstm = self.embedding_net.assemble_sample(processed_sample)\n",
    "        x_lstm = sample\n",
    "\n",
    "        # Forward pass through the LSTM\n",
    "        out, _ = self.lstm(x_lstm)\n",
    "        \n",
    "        # Extract sequence of interest\n",
    "        out = out[:, -self.predict_last_n:, :]\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # Probabilistic things\n",
    "        w = self.fc_weights(out)\n",
    "\n",
    "        params = self.fc_params(out)\n",
    "        match self.distribution:\n",
    "            case Distribution.GAUSSIAN:\n",
    "                loc, scale = params.chunk(2, dim=-1)\n",
    "                scale = F.softplus(scale)\n",
    "                params = {\"loc\": loc, \"scale\": scale}\n",
    "            case Distribution.LAPLACIAN:\n",
    "                loc, scale, kappa = params.chunk(3, dim=-1)\n",
    "                scale = F.softplus(scale)\n",
    "                kappa = F.softplus(kappa)\n",
    "                params = {\"loc\": loc, \"scale\": scale, \"kappa\": kappa}\n",
    "        params = {k: v.reshape(v.shape[0], v.shape[1], self.num_components, self.output_features) for k, v in params.items()}\n",
    "        \n",
    "        return {\"params\": params, \"weights\": w}\n",
    "    \n",
    "    def mean(self, x):\n",
    "        params, w = self(x).values()\n",
    "        match self.distribution:\n",
    "            case Distribution.GAUSSIAN:\n",
    "                mean = params[\"loc\"]\n",
    "            case Distribution.LAPLACIAN:\n",
    "                loc, scale, kappa = params.values()\n",
    "                mean = loc + scale * (1 - kappa.pow(2)) / kappa\n",
    "        mean = (mean * w).sum(axis=-2)\n",
    "        return mean\n",
    "    \n",
    "    def sample(self, x, num_samples):\n",
    "        params, w = self(x).values()\n",
    "        num_batches, sequence_length, num_components, num_targets = next(iter(params.values())).shape\n",
    "        match self.distribution:\n",
    "            case Distribution.GAUSSIAN:\n",
    "                loc, scale = params.values()\n",
    "                \n",
    "                samples = torch.randn(num_batches, sequence_length, num_components, num_samples, num_targets).to(x.device)\n",
    "            case Distribution.LAPLACIAN:\n",
    "                loc, scale, kappa = params.values()\n",
    "\n",
    "                u = torch.rand(num_batches, sequence_length, num_components, num_samples, num_targets).to(x.device)\n",
    "\n",
    "                # Sampling left or right of the mode?\n",
    "                kappa = kappa.unsqueeze(-2).repeat((1, 1, 1, num_samples, 1))\n",
    "                p_at_mode = kappa**2 / (1 + kappa**2)\n",
    "\n",
    "                mask = u < p_at_mode\n",
    "\n",
    "                samples = torch.zeros_like(u)\n",
    "\n",
    "                samples[mask] = kappa[mask] * torch.log(u[mask] * (1 + kappa[mask].pow(2)) / kappa[mask].pow(2)) # Left side\n",
    "                samples[~mask] = -1 * torch.log((1 - u[~mask]) * (1 + kappa[~mask].pow(2))) / kappa[~mask] # Right side\n",
    "\n",
    "        # Forgive me father for I have sinned.\n",
    "        \n",
    "        # samples: [num_batches, sequence_length, num_components, num_samples, output_features]\n",
    "        # loc, scale: [num_batches, sequence_length, num_components, output_features]\n",
    "        samples = samples * scale.unsqueeze(-2) + loc.unsqueeze(-2)  # [num_batches, sequence_length, num_components, num_samples, output_features]\n",
    "\n",
    "        # Select samples according to weights\n",
    "        # w: [num_batches, sequence_length, num_components, output_features]\n",
    "        # Reshape w to [num_batches * sequence_length * output_features, num_components] for multinomial\n",
    "        w_reshaped = w.permute(0, 1, 3, 2).reshape(-1, w.size(2))  # [num_batches * sequence_length * output_features, num_components]\n",
    "        indices = torch.multinomial(w_reshaped, num_samples, replacement=True)  # [num_batches * sequence_length * output_features, num_samples]\n",
    "\n",
    "        # Reshape indices back to proper dimensions\n",
    "        indices = indices.view(num_batches, sequence_length, num_targets, num_samples)  # [num_batches, sequence_length, output_features, num_samples]\n",
    "        indices = indices.permute(0, 1, 3, 2)  # [num_batches, sequence_length, num_samples, output_features]\n",
    "        indices = indices.unsqueeze(2)  # [num_batches, sequence_length, 1, num_samples, output_features]\n",
    "\n",
    "        # Now gather from the num_components dimension (dim=2)\n",
    "        samples = torch.gather(samples, dim=2, index=indices)  # [num_batches, sequence_length, 1, num_samples, output_features]\n",
    "        samples = samples.squeeze(2)  # [num_batches, sequence_length, num_samples, output_features]\n",
    "        \n",
    "        return samples\n",
    "   \n",
    "    def _calc_cdf(self, x, xi):\n",
    "        \"\"\"\n",
    "        Calculate the mixture CDF at points `xi`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor [B, L, I].\n",
    "        xi : torch.Tensor\n",
    "            Evaluation points [B, T, Q, D].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Mixture CDF values [B, N, Q, D].\n",
    "        \"\"\"\n",
    "        xi = xi.unsqueeze(-2) # [B, N, 1, T]\n",
    "\n",
    "        params, weights = self(x).values()\n",
    "\n",
    "        match self.distribution:\n",
    "            case Distribution.GAUSSIAN:\n",
    "                loc, scale = params.values() # loc: [B, N, K, T]\n",
    "                z = (xi - loc) / (scale * math.sqrt(2)) \n",
    "                cdf = 0.5 * (1 + torch.erf(z))\n",
    "\n",
    "            case Distribution.LAPLACIAN:\n",
    "                loc, scale, kappa = params.values()\n",
    "                z = (xi - loc) / scale\n",
    "                mask = (z >= 0)\n",
    "                cdf = torch.zeros_like(z)\n",
    "                cdf[mask] = 1 - (1 / (1 + kappa[mask].pow(2))) * torch.exp(-1 * kappa[mask] * z[mask])\n",
    "                cdf[~mask] = (kappa[~mask].pow(2) / (1 + kappa[~mask].pow(2))) * torch.exp(z[~mask] / kappa[~mask])\n",
    "\n",
    "        # Mix CDF (weighted mixture over components)\n",
    "        cdf = (weights * cdf).sum(dim=-2)  # [B, N, T]\n",
    "        return cdf\n",
    "    \n",
    "    def _calc_logpdf(self, x, xi):\n",
    "        \"\"\"\n",
    "        Calculate the density of `xi` in the mixture PDF of `x`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor \n",
    "            Tensor of shape [B, L, I].\n",
    "        xi : torch.Tensor\n",
    "            The points at which to evaluate the PDF. Tensor of shape [B, N, T].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The PDF values at `xi`. Tensor of shape [B, N, T].\n",
    "        \"\"\"\n",
    "\n",
    "        xi = xi.unsqueeze(-2) # [B, N, 1, T]\n",
    "\n",
    "        params, weights = self(x).values() # loc: [B, N, K, T]\n",
    "        match self.distribution:\n",
    "            case Distribution.GAUSSIAN:\n",
    "                loc, scale = params.values()\n",
    "                scale = torch.clamp(scale, min=1e-6)\n",
    "                p = (xi - loc) / scale\n",
    "                log_p = -0.5 * p.pow(2) - torch.log(scale) - 0.5 * torch.log(2 * math.pi)\n",
    "\n",
    "            case Distribution.LAPLACIAN:\n",
    "                loc, scale, kappa = params.values()\n",
    "                scale = torch.clamp(scale, min=1e-6)\n",
    "                kappa = torch.clamp(kappa, min=1e-6)\n",
    "                \n",
    "\n",
    "                p = (xi - loc) / scale\n",
    "                mask = (p >= 0)\n",
    "\n",
    "                log_p = torch.zeros_like(p)\n",
    "\n",
    "                log_p[mask] = -1 * p[mask] * kappa[mask]\n",
    "                log_p[~mask] = p[~mask] / kappa[~mask]\n",
    "\n",
    "                log_p = log_p - torch.log(kappa + 1 / kappa) - torch.log(scale)\n",
    "\n",
    "        log_w = torch.log(torch.clamp(weights, min=1e-10))\n",
    "        log_p = torch.logsumexp(log_p + log_w, dim=-2) # [B, N, T]\n",
    "    \n",
    "        return log_p\n",
    "\n",
    "    def quantile(self, x, q: list[float], max_iter: int = 50, tol: float = 1e-6):\n",
    "        out = []\n",
    "\n",
    "        # Solve one quantile at a time\n",
    "        for qi in q:\n",
    "            # Mean as the initial guess\n",
    "            xi = self.mean(x)  # [B, N, T]\n",
    "            for _ in range(max_iter):\n",
    "                pdf = self._calc_logpdf(x, xi).exp()   # [B, N, T]\n",
    "                cdf = self._calc_cdf(x, xi)            # [B, N, T]\n",
    "\n",
    "                # Newton step\n",
    "                delta = (cdf - qi) / (pdf + 1e-12)     # [B, N, T]\n",
    "                new = xi - delta\n",
    "\n",
    "                # Convergence check\n",
    "                if delta.abs().max() < tol:\n",
    "                    xi = new\n",
    "                    break\n",
    "\n",
    "                xi = new\n",
    "\n",
    "            out.append(xi)\n",
    "\n",
    "        # Stack quantiles → [B, N, Q, T]\n",
    "        return torch.stack(out, dim=2)\n",
    "\n",
    "model = LSTMMDN(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a661676e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 15, 3, 2])\n",
      "torch.Size([256, 15, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "x_lstm = torch.randn(256, 365, 33)\n",
    "\n",
    "out = model(x_lstm)\n",
    "print(out[\"params\"][\"loc\"].shape)\n",
    "\n",
    "print(out[\"weights\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dd04562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 15, 2])\n"
     ]
    }
   ],
   "source": [
    "mean = model.mean(x_lstm)\n",
    "print(mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8735233e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 15, 2])\n",
      "torch.Size([256, 15, 2])\n"
     ]
    }
   ],
   "source": [
    "xi = torch.randn(256, 15, 2)\n",
    "\n",
    "logpdf = model._calc_logpdf(x_lstm, xi)\n",
    "print(logpdf.shape)\n",
    "cdf = model._calc_cdf(x_lstm, xi)\n",
    "print(cdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "794df6eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "q = [0.025, 0.5, 0.975]\n",
    "\n",
    "quantiles = model.quantile(x_lstm, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6704133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = model.mean(x_lstm)\n",
    "print(mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade32e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = model.sample(x_lstm, num_samples=667)\n",
    "print(samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a998af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mask(*tensors):\n",
    "    masks = []\n",
    "    for tensor in tensors:\n",
    "        num_dim = tensor.dim()\n",
    "        for _ in range(num_dim - 1):\n",
    "            tensor = tensor.sum(dim=1)\n",
    "        mask = ~tensor.isnan()\n",
    "        masks.append(mask)\n",
    "    mask = torch.stack(masks, dim=1).all(dim=1)\n",
    "\n",
    "    return tuple(tensor[mask] for tensor in tensors)\n",
    "\n",
    "def loss_nll(\n",
    "    params: dict[str, torch.Tensor],\n",
    "    weights: torch.Tensor,\n",
    "    dist: Distribution,\n",
    "    y: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \n",
    "    y = y.unsqueeze(-2)  # [batch_size, sequence_length, 1, output_features]\n",
    "\n",
    "    match dist:\n",
    "        case Distribution.GAUSSIAN:\n",
    "            loc, scale = params.values()\n",
    "            scale = torch.clamp(scale, min=1e-6)\n",
    "            p = (y - loc) / scale\n",
    "            log_p = -0.5 * p.pow(2) - torch.log(scale) - 0.5 * torch.log(2 * math.pi)\n",
    "\n",
    "        case Distribution.LAPLACIAN:\n",
    "            loc, scale, kappa = params.values()\n",
    "            scale = torch.clamp(scale, min=1e-6)\n",
    "            kappa = torch.clamp(kappa, min=1e-6)\n",
    "\n",
    "            p = (y - loc) / scale\n",
    "\n",
    "            mask = (p >= 0)\n",
    "\n",
    "            log_p = torch.zeros_like(p)\n",
    "\n",
    "            log_p[mask] = -1 * p[mask] * kappa[mask]\n",
    "            log_p[~mask] = p[~mask] / kappa[~mask]\n",
    "\n",
    "            log_p = log_p - torch.log(kappa + 1 / kappa) - torch.log(scale)\n",
    "\n",
    "    log_w = torch.log(torch.clamp(weights, min=1e-10))\n",
    "    loss = -torch.logsumexp(log_p + log_w, dim=1)\n",
    "    return loss.mean(dim=(0, 1))\n",
    "\n",
    "y_obs = torch.randn(256, 15, 2)\n",
    "\n",
    "loss = loss_nll(out[\"params\"], out[\"weights\"], model.distribution, y_obs)\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hy2dl (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
