{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "# Import classes and functions from other files\n",
    "from hy2dl.aux_functions.functions_training import nse_basin_averaged\n",
    "from hy2dl.aux_functions.functions_evaluation import forecast_NSE, forecast_PNSE\n",
    "from hy2dl.datasetzoo import get_dataset\n",
    "from hy2dl.modelzoo import get_model\n",
    "from hy2dl.utils.config import Config\n",
    "from hy2dl.utils.optimizer import Optimizer\n",
    "from hy2dl.utils.utils import set_random_seed, upload_to_device, write_report\n",
    "\n",
    "# colorblind friendly palette\n",
    "color_palette = {\"observed\": \"#377eb8\",\"simulated\": \"#4daf4a\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1. Initialize information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary where all the information will be stored\n",
    "experiment_settings = {}\n",
    "\n",
    "# Experiment name\n",
    "experiment_settings[\"experiment_name\"] = \"TestErase\"\n",
    "\n",
    "# paths to access the information\n",
    "experiment_settings[\"path_data\"] = \"../data/luxemburg\"\n",
    "experiment_settings[\"path_entities_training\"] = \"../data/luxemburg/basins_lux_25.txt\"\n",
    "experiment_settings[\"path_entities_validation\"] = \"../data/luxemburg/basins_lux_27.txt\"\n",
    "experiment_settings[\"path_entities_testing\"] = \"../data/luxemburg/basins_lux_27.txt\"\n",
    "\n",
    "# dynamic forcings for hindcast period\n",
    "experiment_settings[\"dynamic_input\"] = [\"precipitation\",\"rel_humidity\",\"temperature\",\"radiation\",\"pressure\",\"wind\"]\n",
    "# dymamic forcings for forecast period\n",
    "experiment_settings[\"forecast_input\"] = [\"precipitation\",\"rel_humidity\",\"temperature\",\"radiation\",\"pressure\",\"wind\"]\n",
    "# target\n",
    "experiment_settings[\"target\"] = [\"discharge\"]\n",
    "\n",
    "# static attributes that will be used. If one is not using static_inputs, initialize the variable as an empty list.\n",
    "experiment_settings[\"static_input\"] = [\"MNQ\",\"MQ\",\"MHQ\",\"area\",\"elev_mean\",\"slope_mean\",\"p_mean\",\"pet_mean\",\"aridity\",\"t_mean\",\n",
    "                                       \"snow_cover\",\"forest_frac\",\"crop_frac\",\"urban_frac\",\"clay_frac\",\"silt_frac\",\"sand_frac\"]\n",
    "\n",
    "# time periods\n",
    "experiment_settings[\"training_period\"] = [\"2002-01-01 00:00:00\", \"2017-12-31 23:00:00\"] \n",
    "experiment_settings[\"validation_period\"] = [\"2015-01-01 00:00:00\", \"2017-12-31 23:00:00\"]\n",
    "experiment_settings[\"testing_period\"] = [\"2018-01-01 00:00:00\", \"2022-12-31 23:00:00\"]\n",
    "\n",
    "# model configuration\n",
    "experiment_settings[\"hidden_size\"]= 128\n",
    "experiment_settings[\"batch_size_training\"]= 256\n",
    "experiment_settings[\"batch_size_evaluation\"]= 1024\n",
    "experiment_settings[\"epochs\"]= 10\n",
    "experiment_settings[\"dropout_rate\"]= 0.4\n",
    "experiment_settings[\"learning_rate\"]= {1: 1e-3, 5: 5e-4, 7: 1e-4}\n",
    "experiment_settings[\"validate_every\"]= 1\n",
    "experiment_settings[\"validate_n_random_basins\"]= -1\n",
    "\n",
    "experiment_settings[\"seq_length_hindcast\"]= 364*24\n",
    "experiment_settings[\"custom_seq_processing\"] = {\"hc_1D\": {\"n_steps\": 351,\"freq_factor\": 24,},\n",
    "                                                \"hc_1h\": {\"n_steps\": (365 - 352) * 24, \"freq_factor\": 1}}\n",
    "\n",
    "experiment_settings[\"seq_length_forecast\"]= 24\n",
    "experiment_settings[\"predict_last_n\"] = 24\n",
    "\n",
    "experiment_settings[\"unique_prediction_blocks\"] = False\n",
    "experiment_settings[\"dynamic_embedding\"] = {\"hiddens\": [10, 10, 6]}\n",
    "experiment_settings[\"custom_seq_processing_flag\"] = True\n",
    "\n",
    "\n",
    "# device to train the model\n",
    "experiment_settings[\"device\"] = \"gpu\"\n",
    "experiment_settings[\"num_workers\"] = 4\n",
    "\n",
    "# define random seed\n",
    "experiment_settings[\"random_seed\"] = 110\n",
    "\n",
    "# dataset\n",
    "experiment_settings[\"dataset\"] = \"luxemburg\"\n",
    "\n",
    "# model\n",
    "experiment_settings[\"model\"] = \"CudaLSTM\"\n",
    "experiment_settings[\"initial_forget_bias\"] = 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize experiment configuration\n",
    "config = Config(experiment_settings)\n",
    "config.dump() # save the configuration to a YAML file for reproducibility\n",
    "\n",
    "# Get dataset class\n",
    "Dataset = get_dataset(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2. Create datasets and dataloaders used to train/validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset training\n",
    "training_dataset = Dataset(cfg= config, time_period= \"training\")\n",
    "\n",
    "training_dataset.calculate_basin_std()\n",
    "training_dataset.calculate_global_statistics(save_scaler=True)\n",
    "training_dataset.standardize_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader training\n",
    "train_loader = DataLoader(dataset=training_dataset,\n",
    "                          batch_size=config.batch_size_training,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True,\n",
    "                          collate_fn=training_dataset.collate_fn,\n",
    "                          num_workers=config.num_workers)\n",
    "\n",
    "# Print details of a loaderÂ´s sample to see that our format is correct\n",
    "print(\"Number of batches in training: \", len(train_loader))\n",
    "print(\"\\nSample batch details\")\n",
    "print(f\"\\n{'Key':<30} | {'Shape':<20}\")\n",
    "print(\"-\" * 55)\n",
    "# Loop through the sample dictionary and print the shape of each element\n",
    "for key, value in next(iter(train_loader)).items():\n",
    "    if key.startswith((\"x_d\", \"x_conceptual\")):\n",
    "        print(f\"{key}\")\n",
    "        for i, v in value.items():\n",
    "            print(f\"{'':<4}{i:<26} | {str(v.shape):<20}\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"{key:<30} | {str(value.shape):<20}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In evaluation (validation and testing) we will create an individual dataset per basin\n",
    "entities_ids = np.loadtxt(config.path_entities_validation, dtype=\"str\").tolist()\n",
    "entities_ids = [entities_ids] if isinstance(entities_ids, str) else entities_ids\n",
    "validation_dataset = {}\n",
    "for entity in entities_ids:\n",
    "    dataset = Dataset(cfg= config, \n",
    "                      time_period= \"validation\",\n",
    "                      check_NaN=False,\n",
    "                      entities_ids=entity)\n",
    "    \n",
    "    dataset.scaler = training_dataset.scaler\n",
    "    dataset.standardize_data(standardize_output=False)\n",
    "    validation_dataset[entity] = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "set_random_seed(cfg=config)\n",
    "model = get_model(config).to(config.device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = Optimizer(cfg=config, model=model) \n",
    "\n",
    "training_time = time.time()\n",
    "# Loop through the different epochs\n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    total_loss = []\n",
    "    # Training -------------------------------------------------------------------------------------------------------\n",
    "    model.train()\n",
    "    for idx, sample in enumerate(train_loader):\n",
    "\n",
    "        # reach maximum iterations per epoch\n",
    "        if config.max_updates_per_epoch is not None and idx >= config.max_updates_per_epoch:\n",
    "            break\n",
    "\n",
    "        sample = upload_to_device(sample, config.device)  # upload tensors to device\n",
    "        optimizer.optimizer.zero_grad()  # sets gradients of weigths and bias to zero\n",
    "        pred = model(sample)  # forward call\n",
    "\n",
    "        loss = nse_basin_averaged(y_sim=pred[\"y_hat\"], \n",
    "                                  y_obs=sample[\"y_obs\"], \n",
    "                                  per_basin_target_std=sample[\"std_basin\"])\n",
    "\n",
    "        loss.backward()  # backpropagates\n",
    "        \n",
    "        optimizer.clip_grad_and_step(epoch, idx) # clip gradients and update weights\n",
    "        \n",
    "        total_loss.append(loss.item())\n",
    "\n",
    "        # remove from cuda\n",
    "        del sample, pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # training report\n",
    "    report = f'Epoch: {epoch:<2} | Loss training: {\"%.3f \"% (np.mean(total_loss))}'\n",
    "\n",
    "    # Validation -----------------------------------------------------------------------------------------------------\n",
    "    if epoch % config.validate_every == 0:\n",
    "        model.eval()\n",
    "        validation_results = {}\n",
    "        with torch.no_grad():\n",
    "            # If we define validate_n_random_basins as 0 or negative, we take all the basins\n",
    "            if config.validate_n_random_basins <= 0:\n",
    "                validation_basin_ids = validation_dataset.keys()\n",
    "            else:\n",
    "                keys = list(validation_dataset.keys())\n",
    "                validation_basin_ids = random.sample(keys, config.validate_n_random_basins)\n",
    "\n",
    "            # Go through each basin that will be used for validation\n",
    "            for basin in validation_basin_ids:\n",
    "                loader = DataLoader(\n",
    "                    dataset=validation_dataset[basin],\n",
    "                    batch_size=config.batch_size_evaluation,\n",
    "                    shuffle=False,\n",
    "                    drop_last=False,\n",
    "                    collate_fn=validation_dataset[basin].collate_fn,\n",
    "                    num_workers=config.num_workers\n",
    "                )\n",
    "\n",
    "                dates, simulated_values, observed_values = [], [], []\n",
    "                for i, sample in enumerate(loader):\n",
    "                    sample = upload_to_device(sample, config.device)  # upload tensors to device\n",
    "                    pred = model(sample)\n",
    "                    # backtransformed information\n",
    "                    y_sim = pred[\"y_hat\"] * dataset.scaler[\"y_std\"].to(config.device) + dataset.scaler[\"y_mean\"].to(config.device)\n",
    "\n",
    "                    # Join the results from the different batches\n",
    "                    dates.extend(sample[\"date_issue_fc\"])\n",
    "                    observed_values.extend(sample[\"persistent_q\"].cpu().detach().numpy())\n",
    "                    simulated_values.append(y_sim.cpu().detach().numpy())\n",
    "                    if i == len(loader) - 1:\n",
    "                        dates.extend(sample[\"date\"][-1, :])\n",
    "                        observed_values.extend(sample[\"y_obs\"][-1, :].cpu().detach().numpy())\n",
    "\n",
    "                    # remove from cuda\n",
    "                    del sample, pred, y_sim\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # Construct dataframe with observed and simulated values\n",
    "                df = pd.DataFrame(index=dates)\n",
    "                df[\"Observed\"] = np.concatenate(observed_values, axis=0)\n",
    "                y_sim = np.squeeze(np.concatenate(simulated_values, axis=0), -1)\n",
    "                y_sim = np.concatenate((y_sim, np.full([y_sim.shape[1], y_sim.shape[1]], np.nan)), axis=0)\n",
    "                df[[f\"lead_time_{i + 1}\" for i in range(y_sim.shape[1])]] = y_sim\n",
    "                \n",
    "                validation_results[basin] = df\n",
    "                \n",
    "            # average loss validation\n",
    "            loss_validation = forecast_NSE(results=validation_results).median().mean()\n",
    "            report += f'| NSE validation: {\"%.3f \"% (loss_validation)}'\n",
    "\n",
    "    # save model after every epoch\n",
    "    torch.save(model.state_dict(), config.path_save_folder / \"model\" / f\"model_epoch_{epoch}\")\n",
    "\n",
    "    # print epoch report\n",
    "    report += (\n",
    "        f'| Epoch time: {\"%.1f \"% (time.time()-epoch_start_time)} s | '\n",
    "        f'LR:{\"%.5f \"% (optimizer.optimizer.param_groups[0][\"lr\"])}'\n",
    "    )\n",
    "    print(report)\n",
    "    write_report(cfg = config, text=report)\n",
    "    # modify learning rate\n",
    "    optimizer.update_optimizer_lr(epoch=epoch)\n",
    "\n",
    "# print final report\n",
    "report = f'Total training time: {\"%.1f \"% (time.time()-training_time)} s'\n",
    "print(report)\n",
    "write_report(cfg = config, text=report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case I already trained an LSTM I can re-construct the model\n",
    "#model = get_model(config).to(config.device)\n",
    "#model.load_state_dict(torch.load(config.path_save_folder / \"model\" / \"model_epoch_5\", map_location=config.device))\n",
    "\n",
    "# We can read the training scaler or read a previously stored one\n",
    "scaler = training_dataset.scaler\n",
    "#with open(config.path_save_folder / \"scaler.pickle\", \"rb\") as file:\n",
    "#    scaler = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In evaluation (validation and testing) we will create an individual dataset per basin. This will give us more \n",
    "# flexibility\n",
    "entities_ids = np.loadtxt(config.path_entities_testing, dtype=\"str\").tolist()\n",
    "entities_ids = [entities_ids] if isinstance(entities_ids, str) else entities_ids\n",
    "\n",
    "testing_dataset = {}\n",
    "for entity in entities_ids:\n",
    "    dataset = Dataset(cfg= config, \n",
    "                      time_period= \"testing\",\n",
    "                      check_NaN=False,\n",
    "                      entities_ids=entity)\n",
    "\n",
    "    dataset.scaler = scaler\n",
    "    dataset.standardize_data(standardize_output=False)\n",
    "    testing_dataset[entity] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_results = {}\n",
    "with torch.no_grad():\n",
    "    for basin in testing_dataset:\n",
    "        loader = DataLoader(\n",
    "            dataset=testing_dataset[basin],\n",
    "            batch_size=config.batch_size_evaluation,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            collate_fn=testing_dataset[basin].collate_fn,\n",
    "            num_workers=config.num_workers\n",
    "        )\n",
    "\n",
    "        dates, simulated_values, observed_values = [], [], []\n",
    "        for i, sample in enumerate(loader):\n",
    "            sample = upload_to_device(sample, config.device)  # upload tensors to device\n",
    "            pred = model(sample)\n",
    "            # backtransformed information\n",
    "            y_sim = pred[\"y_hat\"] * dataset.scaler[\"y_std\"].to(config.device) + dataset.scaler[\"y_mean\"].to(config.device)\n",
    "\n",
    "            # Join the results from the different batches\n",
    "            dates.extend(sample[\"date_issue_fc\"])\n",
    "            observed_values.extend(sample[\"persistent_q\"].cpu().detach().numpy())\n",
    "            simulated_values.append(y_sim.cpu().detach().numpy())\n",
    "            if i == len(loader) - 1:\n",
    "                dates.extend(sample[\"date\"][-1, :])\n",
    "                observed_values.extend(sample[\"y_obs\"][-1, :].cpu().detach().numpy())\n",
    "\n",
    "            # remove from cuda\n",
    "            del sample, pred, y_sim\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Construct dataframe with observed and simulated values\n",
    "        df = pd.DataFrame(index=dates)\n",
    "        df[\"Observed\"] = np.concatenate(observed_values, axis=0)\n",
    "        y_sim = np.squeeze(np.concatenate(simulated_values, axis=0), -1)\n",
    "        y_sim = np.concatenate((y_sim, np.full([y_sim.shape[1], y_sim.shape[1]], np.nan)), axis=0)\n",
    "        df[[f\"lead_time_{i + 1}\" for i in range(y_sim.shape[1])]] = y_sim\n",
    "\n",
    "        # Save the dataframe in a basin-indexed dictionary\n",
    "        test_results[basin] = df\n",
    "\n",
    "# Save results as a pickle file\n",
    "with open(config.path_save_folder / \"test_results.pickle\", \"wb\") as f:\n",
    "    pickle.dump(test_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 5. Initial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results testing\n",
    "df_NSE = forecast_NSE(results=test_results)\n",
    "\n",
    "#figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for index, row in df_NSE.iterrows():\n",
    "    plt.plot(np.arange(len(row))+1, df_NSE.T[index], label=f\"B{index:<2} ({row.mean():.2f})\")\n",
    "\n",
    "plt.ylabel(\"NSE\", fontsize=16, fontweight=\"bold\")\n",
    "plt.xlabel(\"Lead time [h]\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tick_params(axis=\"both\", labelsize=14)\n",
    "plt.title(\"NSE per lead time\", fontsize=18, fontweight=\"bold\")\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1.01, 0.5), ncol=1, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize forecast for basin and period of interest\n",
    "basin_of_interest = \"17\"\n",
    "period_of_interest = [\"2020-02-02 00:00:00\", \"2020-02-06 23:00:00\"]\n",
    "\n",
    "# Filter the results\n",
    "df_period_of_interest = test_results[basin_of_interest].loc[period_of_interest[0] : period_of_interest[1], :]\n",
    "\n",
    "# Create figure\n",
    "fig, ax1 = plt.subplots(figsize=(15, 7.5))\n",
    "\n",
    "# Observe series\n",
    "ax1.plot(df_period_of_interest[\"Observed\"], label=\"Observed discharge\", color=color_palette[\"observed\"], linewidth=3, \n",
    "         marker=\"o\")\n",
    "\n",
    "# Simulated forecasted series\n",
    "for i in range(0, df_period_of_interest.shape[0]-1, 1):\n",
    "    time_slide = pd.date_range(\n",
    "        start=df_period_of_interest.index[i+1], periods=df_period_of_interest.shape[1] - 1, freq=\"h\"\n",
    "    )\n",
    "\n",
    "    forecast = df_period_of_interest.iloc[i, 1:].values\n",
    "    ax1.plot(time_slide, forecast, alpha=0.5, linestyle=\"--\")\n",
    "\n",
    "\n",
    "# Format plot\n",
    "ax1.set_xlabel(\"Date\", fontsize=14, fontweight=\"bold\")\n",
    "ax1.tick_params(axis=\"x\", labelsize=14)\n",
    "ax1.set_ylabel(\"Discharge [mm/h]\", fontsize=16, fontweight=\"bold\")\n",
    "ax1.tick_params(axis=\"y\", labelsize=14)\n",
    "ax1.set_title(\"Forecasted discharge\", fontsize=18, fontweight=\"bold\")\n",
    "\n",
    "# Save the figure\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"../results/LuxCamels_seed_110/forecasted_discharge.png\", dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m111"
  },
  "kernelspec": {
   "display_name": "hy2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
